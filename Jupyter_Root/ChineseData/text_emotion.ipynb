{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c96573a",
   "metadata": {},
   "source": [
    "# 中文评论情感分析 -- 正面评论和负面评论的分类\n",
    "\n",
    "本次作业，我基于中文分词和TF-IDF算法，利用jieba库和机器学习sklearn以及keras库，实现了一个中文评论情感分析的模型，该模型能够以超过90%的准确率判断出一段评论是积极的还是消极的。\n",
    "\n",
    "本次作业的数据集来自网上提取的苹果店铺评论，停用词表采用哈工大中文停用词表。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40894e39",
   "metadata": {},
   "source": [
    "## 0 导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09f6ced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import model_selection \n",
    "from sklearn import preprocessing\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.optimizers import Adam\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4e7638",
   "metadata": {},
   "source": [
    "## 1 数据清洗"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e04fe2c",
   "metadata": {},
   "source": [
    "### 1.1 原始数据做分词及剔除停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15e8f105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建停用词列表\n",
    "def stopwordslist():\n",
    "    stopwords = [line.strip() for line in open(r'E:\\Data\\ChineseData\\textemotion/HGD_StopWords.txt',encoding='UTF-8').readlines()]\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f06212ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对句子进行中文分词\n",
    "def seg_depart(sentence):\n",
    "    # 对文档中的每一行进行中文分词\n",
    "    sentence_depart = jieba.cut(sentence.strip())\n",
    "    # 引进停用词列表\n",
    "    stopwords = stopwordslist()\n",
    "    # 输出结果为outstr\n",
    "    outstr = ''\n",
    "    # 去停用词\n",
    "    for word in sentence_depart:\n",
    "        if word not in stopwords:\n",
    "            if word != '\\t':\n",
    "                outstr += word\n",
    "                outstr += \" \"\n",
    "    return outstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "676cf2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一共处理了 7848 条数据\n",
      "删除停用词和分词成功！！！\n"
     ]
    }
   ],
   "source": [
    "filename = r'E:\\Data\\ChineseData\\textemotion/ALL_Comment.txt' # 原始数据\n",
    "outfilename = r'E:\\Data\\ChineseData\\textemotion/stop_seg_word.txt' # 经过停用词表筛选之后的数据\n",
    "inputs = open(filename, 'r', encoding='UTF-8')\n",
    "outputs=open(outfilename, 'w', encoding='UTF-8')\n",
    "# 将输出结果写入out中\n",
    "count=0\n",
    "for line in inputs:\n",
    "    line_seg = seg_depart(line)\n",
    "    outputs.writelines(line_seg + '\\n')\n",
    "    count=count+1\n",
    "print(\"一共处理了\",count,\"条数据\")\n",
    "outputs.close()\n",
    "inputs.close()\n",
    "print(\"删除停用词和分词成功！！！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0a00b9",
   "metadata": {},
   "source": [
    "### 1.2 将数据和标签对应，存进csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d20436ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7848\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>评论</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿ 真机 很漂亮 体验 看 评论 说 发烫 情况 几天 摄像头 有点 现在 挺 待机 感觉 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>外观 好看 黄色 特别 喜欢 裸机 非常 好看 旁边 黑边 略微 习惯 还好 面部 解锁 非...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>手感 光滑 圆润 质感 不像 中 略厚 一点点 适合 女孩子 无碍 诟病 屏幕 黑边 我觉 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>这次 iphone11 值得 购买 拍摄 方面 很大 提升 夜景 模式 测试 看 原图 噪点...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>手机 不错 毕竟 系统 强大 拍照 效果 音效 都 提升 操作 顺滑 待机 期间 长刚 激活...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7843</th>\n",
       "      <td>京东 老 客服 这次 体验 非常 愉快 标签 损坏 重贴 痕迹 重度 怀疑 退换货 翻新 机...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7844</th>\n",
       "      <td>买回来 没 仔细 看 带上 买回来 软套 前 几天 打开 一看 背面 有个 厘米 口子 非常...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7845</th>\n",
       "      <td>差评 差评 一点 都 不好 怀疑 翻新 拆封 机 左边 缝隙 右边 无比 贴合 客服 还 说...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7846</th>\n",
       "      <td>真是 服 京东 全 假货 真信   买 手里 激活 超过 天   白屏 开机 充电   京东...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7847</th>\n",
       "      <td>白条 加免 减 手机 真的 很差 像素 一款 买 苹果 都 差 雪花 米点 一大堆 八十年代...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7848 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     评论\n",
       "0     ﻿ 真机 很漂亮 体验 看 评论 说 发烫 情况 几天 摄像头 有点 现在 挺 待机 感觉 ...\n",
       "1     外观 好看 黄色 特别 喜欢 裸机 非常 好看 旁边 黑边 略微 习惯 还好 面部 解锁 非...\n",
       "2     手感 光滑 圆润 质感 不像 中 略厚 一点点 适合 女孩子 无碍 诟病 屏幕 黑边 我觉 ...\n",
       "3     这次 iphone11 值得 购买 拍摄 方面 很大 提升 夜景 模式 测试 看 原图 噪点...\n",
       "4     手机 不错 毕竟 系统 强大 拍照 效果 音效 都 提升 操作 顺滑 待机 期间 长刚 激活...\n",
       "...                                                 ...\n",
       "7843  京东 老 客服 这次 体验 非常 愉快 标签 损坏 重贴 痕迹 重度 怀疑 退换货 翻新 机...\n",
       "7844  买回来 没 仔细 看 带上 买回来 软套 前 几天 打开 一看 背面 有个 厘米 口子 非常...\n",
       "7845  差评 差评 一点 都 不好 怀疑 翻新 拆封 机 左边 缝隙 右边 无比 贴合 客服 还 说...\n",
       "7846  真是 服 京东 全 假货 真信   买 手里 激活 超过 天   白屏 开机 充电   京东...\n",
       "7847  白条 加免 减 手机 真的 很差 像素 一款 买 苹果 都 差 雪花 米点 一大堆 八十年代...\n",
       "\n",
       "[7848 rows x 1 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "#将txt文件中的数据按行写入csv文件\n",
    "with open(r'E:\\Data\\ChineseData\\textemotion/stop_seg_word.txt', encoding='utf-8') as f:\n",
    "    line = f.readlines()\n",
    "    line = [i.strip() for i in line]\n",
    "    print(len(line))\n",
    "#建立评论这一列，将数据写入\n",
    "data['评论'] = line\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74041852",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>评论</th>\n",
       "      <th>评分</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿ 真机 很漂亮 体验 看 评论 说 发烫 情况 几天 摄像头 有点 现在 挺 待机 感觉 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>外观 好看 黄色 特别 喜欢 裸机 非常 好看 旁边 黑边 略微 习惯 还好 面部 解锁 非...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>手感 光滑 圆润 质感 不像 中 略厚 一点点 适合 女孩子 无碍 诟病 屏幕 黑边 我觉 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>这次 iphone11 值得 购买 拍摄 方面 很大 提升 夜景 模式 测试 看 原图 噪点...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>手机 不错 毕竟 系统 强大 拍照 效果 音效 都 提升 操作 顺滑 待机 期间 长刚 激活...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7843</th>\n",
       "      <td>京东 老 客服 这次 体验 非常 愉快 标签 损坏 重贴 痕迹 重度 怀疑 退换货 翻新 机...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7844</th>\n",
       "      <td>买回来 没 仔细 看 带上 买回来 软套 前 几天 打开 一看 背面 有个 厘米 口子 非常...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7845</th>\n",
       "      <td>差评 差评 一点 都 不好 怀疑 翻新 拆封 机 左边 缝隙 右边 无比 贴合 客服 还 说...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7846</th>\n",
       "      <td>真是 服 京东 全 假货 真信   买 手里 激活 超过 天   白屏 开机 充电   京东...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7847</th>\n",
       "      <td>白条 加免 减 手机 真的 很差 像素 一款 买 苹果 都 差 雪花 米点 一大堆 八十年代...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7848 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     评论 评分\n",
       "0     ﻿ 真机 很漂亮 体验 看 评论 说 发烫 情况 几天 摄像头 有点 现在 挺 待机 感觉 ...  1\n",
       "1     外观 好看 黄色 特别 喜欢 裸机 非常 好看 旁边 黑边 略微 习惯 还好 面部 解锁 非...  1\n",
       "2     手感 光滑 圆润 质感 不像 中 略厚 一点点 适合 女孩子 无碍 诟病 屏幕 黑边 我觉 ...  1\n",
       "3     这次 iphone11 值得 购买 拍摄 方面 很大 提升 夜景 模式 测试 看 原图 噪点...  1\n",
       "4     手机 不错 毕竟 系统 强大 拍照 效果 音效 都 提升 操作 顺滑 待机 期间 长刚 激活...  1\n",
       "...                                                 ... ..\n",
       "7843  京东 老 客服 这次 体验 非常 愉快 标签 损坏 重贴 痕迹 重度 怀疑 退换货 翻新 机...  0\n",
       "7844  买回来 没 仔细 看 带上 买回来 软套 前 几天 打开 一看 背面 有个 厘米 口子 非常...  0\n",
       "7845  差评 差评 一点 都 不好 怀疑 翻新 拆封 机 左边 缝隙 右边 无比 贴合 客服 还 说...  0\n",
       "7846  真是 服 京东 全 假货 真信   买 手里 激活 超过 天   白屏 开机 充电   京东...  0\n",
       "7847  白条 加免 减 手机 真的 很差 像素 一款 买 苹果 都 差 雪花 米点 一大堆 八十年代...  0\n",
       "\n",
       "[7848 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(r'E:\\Data\\ChineseData\\textemotion/All_label.txt', \"r\",encoding='utf-8') as f:\n",
    "    all_label=f.readlines()\n",
    "    print(type(all_label))\n",
    "    print(len(all_label))\n",
    "\n",
    "all_labels=[]\n",
    "for element in all_label:\n",
    "    all_labels.extend(element.split(','))\n",
    "\n",
    "#建立“评分”这一列，将标签写入\n",
    "data['评分'] = all_labels\n",
    "data.to_csv(r'E:\\Data\\ChineseData\\textemotion/reviews_score_update.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e165d8",
   "metadata": {},
   "source": [
    "## 2 数据集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf9af495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>评论</th>\n",
       "      <th>评分</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿ 真机 很漂亮 体验 看 评论 说 发烫 情况 几天 摄像头 有点 现在 挺 待机 感觉 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>外观 好看 黄色 特别 喜欢 裸机 非常 好看 旁边 黑边 略微 习惯 还好 面部 解锁 非...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>手感 光滑 圆润 质感 不像 中 略厚 一点点 适合 女孩子 无碍 诟病 屏幕 黑边 我觉 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>这次 iphone11 值得 购买 拍摄 方面 很大 提升 夜景 模式 测试 看 原图 噪点...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>手机 不错 毕竟 系统 强大 拍照 效果 音效 都 提升 操作 顺滑 待机 期间 长刚 激活...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  评论  评分\n",
       "0  ﻿ 真机 很漂亮 体验 看 评论 说 发烫 情况 几天 摄像头 有点 现在 挺 待机 感觉 ...   1\n",
       "1  外观 好看 黄色 特别 喜欢 裸机 非常 好看 旁边 黑边 略微 习惯 还好 面部 解锁 非...   1\n",
       "2  手感 光滑 圆润 质感 不像 中 略厚 一点点 适合 女孩子 无碍 诟病 屏幕 黑边 我觉 ...   1\n",
       "3  这次 iphone11 值得 购买 拍摄 方面 很大 提升 夜景 模式 测试 看 原图 噪点...   1\n",
       "4  手机 不错 毕竟 系统 强大 拍照 效果 音效 都 提升 操作 顺滑 待机 期间 长刚 激活...   1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将数据进行读取\n",
    "data=pd.read_csv(r'E:\\Data\\ChineseData\\textemotion/reviews_score_update.csv',index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "681155ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7063,) (785,)\n"
     ]
    }
   ],
   "source": [
    "#划分数据集为训练集和预测集\n",
    "train_x,test_x,train_y,test_y=model_selection.train_test_split(data.评论.values.astype('U'),data.评分.values,test_size=0.1,random_state=49)\n",
    " \n",
    "#划分完毕，查看数据形状\n",
    "print(train_x.shape,test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f50f1b",
   "metadata": {},
   "source": [
    "## 3 特征提取 -- TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7d570b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据哈工大中文停用词表进行去停用词\n",
    "def get_stopwords(stop_word_file):\n",
    "    with open(stop_word_file, 'r', encoding='utf-8') as f:\n",
    "        stopwords=f.read()\n",
    "    stopwords_list=stopwords.split('\\n')\n",
    "    custom_stopwords_list=[i for i in stopwords_list]\n",
    "    return custom_stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c343178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获得由停用词组成的列表\n",
    "stop_words_file = r'E:\\Data\\ChineseData\\textemotion/HGD_StopWords.txt'\n",
    "stopwords = get_stopwords(stop_words_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3339e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\21548\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', 'max', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7063, 3597)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "使用TfidfVectorizer()对数据进行特征的提取，投放到不同的模型中进行实验\n",
    "'''\n",
    "#使用TF-IDF进行特征的提取，对分词后的中文语句做向量化。\n",
    "TF_Vec=TfidfVectorizer(max_df=0.8,\n",
    "                       min_df = 3,\n",
    "                       stop_words=frozenset(stopwords)\n",
    "                      )\n",
    "#拟合数据，将数据准转为标准形式，一般使用在训练集中\n",
    "train_x_tfvec=TF_Vec.fit_transform(train_x)\n",
    "#通过中心化和缩放实现标准化，一般使用在测试集中\n",
    "test_x_tfvec=TF_Vec.transform(test_x)\n",
    "train_x_tfvec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e2779b",
   "metadata": {},
   "source": [
    "## 4 搭建神经网络，进行数据训练和预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65b8f210",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(train_x_tfvec.shape[1],)))\n",
    "\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0732b632",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(0.0005),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "346dd428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7063/7063 [==============================] - 1s 201us/step - loss: 0.3026 - accuracy: 0.8569\n",
      "Epoch 2/10\n",
      "7063/7063 [==============================] - 1s 187us/step - loss: 0.0727 - accuracy: 0.9759\n",
      "Epoch 3/10\n",
      "7063/7063 [==============================] - 1s 185us/step - loss: 0.0367 - accuracy: 0.9897\n",
      "Epoch 4/10\n",
      "7063/7063 [==============================] - 1s 191us/step - loss: 0.0212 - accuracy: 0.9943\n",
      "Epoch 5/10\n",
      "7063/7063 [==============================] - 1s 188us/step - loss: 0.0128 - accuracy: 0.9970\n",
      "Epoch 6/10\n",
      "7063/7063 [==============================] - 2s 213us/step - loss: 0.0087 - accuracy: 0.9982\n",
      "Epoch 7/10\n",
      "7063/7063 [==============================] - 1s 204us/step - loss: 0.0061 - accuracy: 0.9984\n",
      "Epoch 8/10\n",
      "7063/7063 [==============================] - 1s 172us/step - loss: 0.0045 - accuracy: 0.9990\n",
      "Epoch 9/10\n",
      "7063/7063 [==============================] - 1s 172us/step - loss: 0.0038 - accuracy: 0.9993\n",
      "Epoch 10/10\n",
      "7063/7063 [==============================] - 1s 172us/step - loss: 0.0032 - accuracy: 0.9992\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_x_tfvec,\n",
    "                    train_y,\n",
    "                    epochs=10,\n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50cfb901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785/785 [==============================] - 0s 65us/step\n",
      "test_acc:  0.9363057613372803\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_x_tfvec,test_y)\n",
    "print('test_acc: ',test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9aa66a",
   "metadata": {},
   "source": [
    "## 5 使用逻辑回归进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3acd6b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最优参数： {'C': 6.7233575364993365, 'penalty': 'l2'}\n",
      "使用TF-IDF提取特征使用逻辑回归,让模型自适应参数，进行模型优化\n",
      "训练集:0.9881070366699702\n",
      "测试集:0.9579617834394905\n",
      "使用模型优化的程序运行时间为 4.295543909072876\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "使用TF_IDF提取的向量当作数据特征传入模型\n",
    "'''\n",
    "#构建模型之前首先将包进行导入\n",
    "start_time=time.time()\n",
    "#创建模型\n",
    "lr = linear_model.LogisticRegression(penalty='l2', C=1, solver='liblinear', max_iter=1000, multi_class='ovr')\n",
    "model = GridSearchCV(lr, cv=3, param_grid={\n",
    "        'C': np.logspace(0, 4, 30),\n",
    "        'penalty': ['l1', 'l2']\n",
    "    })\n",
    "#模型拟合tf-idf拿到的数据\n",
    "model.fit(train_x_tfvec,train_y)\n",
    "#查看模型自己拟合的最优参数\n",
    "print('最优参数：', model.best_params_)\n",
    "#在训练时查看训练集的准确率\n",
    "pre_train_y=model.predict(train_x_tfvec)\n",
    "#在训练集上的正确率\n",
    "train_accracy=accuracy_score(pre_train_y,train_y)\n",
    "#训练结束查看预测 输入验证集查看预测\n",
    "pre_test_y=model.predict(test_x_tfvec)\n",
    "#查看在测试集上的准确率\n",
    "test_accracy = accuracy_score(pre_test_y,test_y)\n",
    "print('使用TF-IDF提取特征使用逻辑回归,让模型自适应参数，进行模型优化\\n训练集:{0}\\n测试集:{1}'.format(train_accracy,test_accracy))\n",
    "end_time=time.time()\n",
    "print(\"使用模型优化的程序运行时间为\",end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a96a1d6",
   "metadata": {},
   "source": [
    "## 6 使用随机森林分类进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fbe92bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用CounterfVectorizer提取特征使用随机森林分类器的准确率\n",
      "训练集:0.9954693473028459\n",
      "测试集:0.9286624203821656\n",
      "使用随机森林分类器的程序运行时间为 0.20747137069702148\n"
     ]
    }
   ],
   "source": [
    "### Random Forest Classifier 随机森林分类器 \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "import time\n",
    "start_time=time.time()\n",
    "#创建模型\n",
    "Rfc = RandomForestClassifier(n_estimators=8)\n",
    "#拟合从CounterfVectorizer拿到的数据\n",
    "Rfc.fit(train_x_tfvec,train_y)\n",
    "#在训练时查看训练集的准确率\n",
    "pre_train_y=Rfc.predict(train_x_tfvec)\n",
    "#在训练集上的正确率\n",
    "train_accracy=accuracy_score(pre_train_y,train_y)\n",
    "#训练结束查看预测 输入测试集查看预测\n",
    "pre_test_y=Rfc.predict(test_x_tfvec)\n",
    "#查看在测试集上的准确率\n",
    "test_accracy = accuracy_score(pre_test_y,test_y)\n",
    "print('使用CounterfVectorizer提取特征使用随机森林分类器的准确率\\n训练集:{0}\\n测试集:{1}'.format(train_accracy,test_accracy))\n",
    "end_time=time.time()\n",
    "print(\"使用随机森林分类器的程序运行时间为\",end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6e0d8c",
   "metadata": {},
   "source": [
    "## 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02e7ac1",
   "metadata": {},
   "source": [
    "本次作业实现了一个准确率较高而分类器，能够判断出一条评论的情感倾向（是正面评价还是负面评价），其中用到了：\n",
    "* jieba库进行中文分词\n",
    "* TF-IDF库进行数据的特征提取\n",
    "* sklearn库进行数据集的划分以及逻辑回归和随机森林的分类\n",
    "* keras库进行神经网络的搭建、训练以及预测\n",
    "\n",
    "对于一份数据集，我进行了三个不同模型的搭建：\n",
    "* 逻辑回归：准确率95.7%\n",
    "* ANN神经网络：准确率93.6%\n",
    "* 随机森林：92.8%\n",
    "\n",
    "结果均能在预测集上表现出很好的效果，实现了中文评论的高准确率情感分析，同时对中文信息处理有了更加深刻的认识。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
